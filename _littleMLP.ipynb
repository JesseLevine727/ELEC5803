{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25bcd4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4872e602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch 01 | Loss: 52.8180\n",
      "Epoch 02 | Loss: 50.9024\n",
      "Epoch 03 | Loss: 48.5352\n",
      "Epoch 04 | Loss: 45.8121\n",
      "Epoch 05 | Loss: 42.7972\n",
      "Epoch 06 | Loss: 39.4486\n",
      "Epoch 07 | Loss: 36.0260\n",
      "Epoch 08 | Loss: 32.4281\n",
      "Epoch 09 | Loss: 28.7962\n",
      "Epoch 10 | Loss: 25.5361\n",
      "Epoch 11 | Loss: 22.8034\n",
      "Epoch 12 | Loss: 20.3684\n",
      "Epoch 13 | Loss: 18.3227\n",
      "Epoch 14 | Loss: 16.6349\n",
      "Epoch 15 | Loss: 15.2956\n",
      "Epoch 16 | Loss: 13.9969\n",
      "Epoch 17 | Loss: 13.0208\n",
      "Epoch 18 | Loss: 12.1856\n",
      "Epoch 19 | Loss: 11.2837\n",
      "Epoch 20 | Loss: 10.6244\n",
      "Epoch 21 | Loss: 9.9670\n",
      "Epoch 22 | Loss: 9.4425\n",
      "Epoch 23 | Loss: 8.8868\n",
      "Epoch 24 | Loss: 8.4056\n",
      "Epoch 25 | Loss: 8.0549\n",
      "Epoch 26 | Loss: 7.6204\n",
      "Epoch 27 | Loss: 7.4250\n",
      "Epoch 28 | Loss: 7.1450\n",
      "Epoch 29 | Loss: 6.8520\n",
      "Epoch 30 | Loss: 6.5092\n",
      "Epoch 31 | Loss: 6.2853\n",
      "Epoch 32 | Loss: 6.0484\n",
      "Epoch 33 | Loss: 5.8125\n",
      "Epoch 34 | Loss: 5.5902\n",
      "Epoch 35 | Loss: 5.4001\n",
      "Epoch 36 | Loss: 5.2923\n",
      "Epoch 37 | Loss: 5.1535\n",
      "Epoch 38 | Loss: 5.0142\n",
      "Epoch 39 | Loss: 4.9102\n",
      "Epoch 40 | Loss: 4.7302\n",
      "\n",
      "Test Accuracy: 0.9667\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Configuration\n",
    "# ============================================================\n",
    "\n",
    "HIDDEN_DIM = 16     # start small, scale to 32 later\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 40\n",
    "LR = 1e-3\n",
    "SEED = 123\n",
    "EXPORT_DIR = \"export\"\n",
    "\n",
    "Q_FRAC = 16\n",
    "Q_SCALE = 1 << Q_FRAC\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# ============================================================\n",
    "# Load Dataset\n",
    "# ============================================================\n",
    "\n",
    "digits = load_digits()\n",
    "X = digits.data.astype(np.float32) / 16.0   # normalize to [0,1]\n",
    "y = digits.target.astype(np.int64)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=SEED, stratify=y\n",
    ")\n",
    "\n",
    "X_train = torch.tensor(X_train)\n",
    "y_train = torch.tensor(y_train)\n",
    "X_test  = torch.tensor(X_test)\n",
    "y_test  = torch.tensor(y_test)\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# Model Definition\n",
    "# ============================================================\n",
    "\n",
    "class TinyMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(64, HIDDEN_DIM)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(HIDDEN_DIM, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x  # raw logits (no softmax)\n",
    "\n",
    "model = TinyMLP()\n",
    "\n",
    "# ============================================================\n",
    "# Training\n",
    "# ============================================================\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "print(\"Training...\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for xb, yb in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1:02d} | Loss: {total_loss:.4f}\")\n",
    "\n",
    "# ============================================================\n",
    "# Evaluation\n",
    "# ============================================================\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(X_test)\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "acc = accuracy_score(y_test.numpy(), preds.numpy())\n",
    "print(f\"\\nTest Accuracy: {acc:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce2933ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch 01 | Loss: 52.8180\n",
      "Epoch 02 | Loss: 50.9024\n",
      "Epoch 03 | Loss: 48.5352\n",
      "Epoch 04 | Loss: 45.8121\n",
      "Epoch 05 | Loss: 42.7972\n",
      "Epoch 06 | Loss: 39.4486\n",
      "Epoch 07 | Loss: 36.0260\n",
      "Epoch 08 | Loss: 32.4281\n",
      "Epoch 09 | Loss: 28.7962\n",
      "Epoch 10 | Loss: 25.5361\n",
      "Epoch 11 | Loss: 22.8034\n",
      "Epoch 12 | Loss: 20.3684\n",
      "Epoch 13 | Loss: 18.3227\n",
      "Epoch 14 | Loss: 16.6349\n",
      "Epoch 15 | Loss: 15.2956\n",
      "Epoch 16 | Loss: 13.9969\n",
      "Epoch 17 | Loss: 13.0208\n",
      "Epoch 18 | Loss: 12.1856\n",
      "Epoch 19 | Loss: 11.2837\n",
      "Epoch 20 | Loss: 10.6244\n",
      "Epoch 21 | Loss: 9.9670\n",
      "Epoch 22 | Loss: 9.4425\n",
      "Epoch 23 | Loss: 8.8868\n",
      "Epoch 24 | Loss: 8.4056\n",
      "Epoch 25 | Loss: 8.0549\n",
      "Epoch 26 | Loss: 7.6204\n",
      "Epoch 27 | Loss: 7.4250\n",
      "Epoch 28 | Loss: 7.1450\n",
      "Epoch 29 | Loss: 6.8520\n",
      "Epoch 30 | Loss: 6.5092\n",
      "Epoch 31 | Loss: 6.2853\n",
      "Epoch 32 | Loss: 6.0484\n",
      "Epoch 33 | Loss: 5.8125\n",
      "Epoch 34 | Loss: 5.5902\n",
      "Epoch 35 | Loss: 5.4001\n",
      "Epoch 36 | Loss: 5.2923\n",
      "Epoch 37 | Loss: 5.1535\n",
      "Epoch 38 | Loss: 5.0142\n",
      "Epoch 39 | Loss: 4.9102\n",
      "Epoch 40 | Loss: 4.7302\n",
      "\n",
      "Test Accuracy: 0.9667\n",
      "\n",
      "Exported weights to export/mlp_digits_q16_16.h\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# ============================================================\n",
    "# Configuration\n",
    "# ============================================================\n",
    "\n",
    "HIDDEN_DIM = 16     # start small, scale to 32 later\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 40\n",
    "LR = 1e-3\n",
    "SEED = 123\n",
    "EXPORT_DIR = \"export\"\n",
    "\n",
    "Q_FRAC = 16\n",
    "Q_SCALE = 1 << Q_FRAC\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# ============================================================\n",
    "# Load Dataset\n",
    "# ============================================================\n",
    "\n",
    "digits = load_digits()\n",
    "X = digits.data.astype(np.float32) / 16.0   # normalize to [0,1]\n",
    "y = digits.target.astype(np.int64)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=SEED, stratify=y\n",
    ")\n",
    "\n",
    "X_train = torch.tensor(X_train)\n",
    "y_train = torch.tensor(y_train)\n",
    "X_test  = torch.tensor(X_test)\n",
    "y_test  = torch.tensor(y_test)\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# Model Definition\n",
    "# ============================================================\n",
    "\n",
    "class TinyMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(64, HIDDEN_DIM)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(HIDDEN_DIM, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x  # raw logits (no softmax)\n",
    "\n",
    "model = TinyMLP()\n",
    "\n",
    "# ============================================================\n",
    "# Training\n",
    "# ============================================================\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "print(\"Training...\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for xb, yb in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1:02d} | Loss: {total_loss:.4f}\")\n",
    "\n",
    "# ============================================================\n",
    "# Evaluation\n",
    "# ============================================================\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(X_test)\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "acc = accuracy_score(y_test.numpy(), preds.numpy())\n",
    "print(f\"\\nTest Accuracy: {acc:.4f}\")\n",
    "\n",
    "# ============================================================\n",
    "# Extract Weights\n",
    "# ============================================================\n",
    "\n",
    "W1 = model.fc1.weight.detach().numpy()     # (HIDDEN_DIM, 64)\n",
    "b1 = model.fc1.bias.detach().numpy()       # (HIDDEN_DIM)\n",
    "W2 = model.fc2.weight.detach().numpy()     # (10, HIDDEN_DIM)\n",
    "b2 = model.fc2.bias.detach().numpy()       # (10)\n",
    "\n",
    "# ============================================================\n",
    "# Quantization to Q16.16\n",
    "# ============================================================\n",
    "\n",
    "def float_to_q16(x):\n",
    "    q = np.round(x * Q_SCALE)\n",
    "    q = np.clip(q, -2**31, 2**31 - 1)\n",
    "    return q.astype(np.int32)\n",
    "\n",
    "W1_q = float_to_q16(W1)\n",
    "b1_q = float_to_q16(b1)\n",
    "W2_q = float_to_q16(W2)\n",
    "b2_q = float_to_q16(b2)\n",
    "\n",
    "# ============================================================\n",
    "# Export Header File\n",
    "# ============================================================\n",
    "\n",
    "os.makedirs(EXPORT_DIR, exist_ok=True)\n",
    "\n",
    "header_path = os.path.join(EXPORT_DIR, \"mlp_digits_q16_16.h\")\n",
    "\n",
    "with open(header_path, \"w\") as f:\n",
    "    f.write(\"// Auto-generated PyTorch Q16.16 MLP weights\\n\\n\")\n",
    "    f.write(\"#pragma once\\n\\n\")\n",
    "    f.write(\"#include <stdint.h>\\n\\n\")\n",
    "    f.write(f\"#define MLP_IN_DIM 64\\n\")\n",
    "    f.write(f\"#define MLP_HID_DIM {HIDDEN_DIM}\\n\")\n",
    "    f.write(f\"#define MLP_OUT_DIM 10\\n\")\n",
    "    f.write(f\"#define MLP_Q_FRAC {Q_FRAC}\\n\\n\")\n",
    "\n",
    "    def write_array(name, arr):\n",
    "        flat = arr.flatten()\n",
    "        f.write(f\"static const int32_t {name}[{len(flat)}] = {{\\n    \")\n",
    "        for i, val in enumerate(flat):\n",
    "            f.write(str(val))\n",
    "            if i != len(flat) - 1:\n",
    "                f.write(\", \")\n",
    "            if (i+1) % 8 == 0:\n",
    "                f.write(\"\\n    \")\n",
    "        f.write(\"\\n};\\n\\n\")\n",
    "\n",
    "    write_array(\"MLP_W1_Q16\", W1_q)\n",
    "    write_array(\"MLP_B1_Q16\", b1_q)\n",
    "    write_array(\"MLP_W2_Q16\", W2_q)\n",
    "    write_array(\"MLP_B2_Q16\", b2_q)\n",
    "\n",
    "print(f\"\\nExported weights to {header_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
